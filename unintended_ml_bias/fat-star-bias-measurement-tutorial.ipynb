{
  "cells": [
    {
      "metadata": {
        "_uuid": "57e422ee36fda0ea9329c23f99a0cbb83d593e26"
      },
      "cell_type": "markdown",
      "source": "# Hands-on Tutorial: Measuring Unintended Bias in Text Classification Models with Real Data"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport pkg_resources\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport scipy.stats as stats\n\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding\nfrom keras.layers import Input\nfrom keras.layers import Conv1D\nfrom keras.layers import MaxPooling1D\nfrom keras.layers import Flatten\nfrom keras.layers import Dropout\nfrom keras.layers import Dense\nfrom keras.optimizers import RMSprop\nfrom keras.models import Model\n\n%matplotlib inline\n\n# autoreload makes it easier to interactively work on code in imported libraries\n%load_ext autoreload\n%autoreload 2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d79a4bc69cbf43b8e41606fc5383bd744ceefdda"
      },
      "cell_type": "markdown",
      "source": "## Load and pre-process data sets"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_v1_df = pd.read_csv('../input/fat-star-tutorial-data/public_train_v1.csv')\nvalidate_df = pd.read_csv('../input/fat-star-tutorial-data/public_validate.csv')\ntest_df = pd.read_csv('../input/fat-star-tutorial-data/public_test.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6a6a3b6efc8e86c9493ec9a4bd5a6e74a79c31db"
      },
      "cell_type": "markdown",
      "source": "Let's examine some rows in these datasets.  Note that columns like toxicity and male are percent scores.\nWe query for \"male >= 0\" to exclude rows where the male identity is not labeled."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "133904af41822603c20d5bfc9c9fb0520e9f94e6",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_v1_df[['toxicity', 'male', 'comment_text']].query('male >= 0').head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fff6ba66d06faae3134e6d11f37b77d2a6db07eb"
      },
      "cell_type": "markdown",
      "source": "We will need to convert toxicity and identity columns to booleans, in order to work with our neural net and metrics calculcations.  For this tutorial, we will consider any value >= 0.5 as True (i.e. a comment should be considered toxic if 50% or more crowd raters labeled it as toxic).  Note that this code also converts missing identity fields to False."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "04f5a81fdfa9b40576962e372a68f75e5b8751f0",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# List all identities\nidentity_columns = [\n    'male', 'female', 'transgender', 'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian',\n    'bisexual', 'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu', 'buddhist',\n    'atheist', 'other_religion', 'black', 'white', 'asian', 'latino', 'other_race_or_ethnicity',\n    'physical_disability', 'intellectual_or_learning_disability', 'psychiatric_or_mental_illness', 'other_disability']\n\ndef convert_to_bool(df, col_name):\n    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n\nfor df in [train_v1_df, validate_df, test_df]:\n    for col in ['toxicity'] + identity_columns:\n        convert_to_bool(df, col)\n    \ntrain_v1_df[['toxicity', 'male', 'comment_text']].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fff9ec5eb25fbeaf604f0c6dd701e239a88cbbdc"
      },
      "cell_type": "markdown",
      "source": "## Create and Train Models\n\nThis code creates and trains a convolutional neural net using the Keras framework.  This neural net accepts a text comment, encoding as a sequence of integers, and outputs a probably that the comment is toxic.  Don't worry if you do not understand all of this code, as we will be treating this neural net as a black box later in the tutorial."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b08dd28d1b574d316c22aec2aec80ddd718cc84a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "MAX_SEQUENCE_LENGTH = 250\nMAX_NUM_WORDS = 10000\nTOXICITY_COLUMN = 'toxicity'\nTEXT_COLUMN = 'comment_text'\nEMBEDDINGS_PATH = '../input/glove6b/glove.6B.100d.txt'\nEMBEDDINGS_DIMENSION = 100\nDROPOUT_RATE = 0.3\nLEARNING_RATE = 0.00005\nNUM_EPOCHS = 10\nBATCH_SIZE = 128",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4543e2c8da329ed84d9b100cdc102214d4ab1a8f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def pad_text(texts, tokenizer):\n    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)\n\ndef train_model(train_df, validate_df, tokenizer):\n    # Prepare data\n    train_text = pad_text(train_df[TEXT_COLUMN], tokenizer)\n    train_labels = to_categorical(train_df[TOXICITY_COLUMN])\n    validate_text = pad_text(validate_df[TEXT_COLUMN], tokenizer)\n    validate_labels = to_categorical(validate_df[TOXICITY_COLUMN])\n\n    # Load embeddings\n    embeddings_index = {}\n    with open(EMBEDDINGS_PATH) as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n\n    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1,\n                                 EMBEDDINGS_DIMENSION))\n    num_words_in_embedding = 0\n    for word, i in tokenizer.word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            num_words_in_embedding += 1\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n\n    # Create model layers.\n    def get_convolutional_neural_net_layers():\n        \"\"\"Returns (input_layer, output_layer)\"\"\"\n        sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n        embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n                                    EMBEDDINGS_DIMENSION,\n                                    weights=[embedding_matrix],\n                                    input_length=MAX_SEQUENCE_LENGTH,\n                                    trainable=False)\n        x = embedding_layer(sequence_input)\n        x = Conv1D(128, 5, activation='relu', padding='same')(x)\n        x = MaxPooling1D(5, padding='same')(x)\n        x = Conv1D(128, 5, activation='relu', padding='same')(x)\n        x = MaxPooling1D(5, padding='same')(x)\n        x = Conv1D(128, 5, activation='relu', padding='same')(x)\n        x = MaxPooling1D(40, padding='same')(x)\n        x = Flatten()(x)\n        x = Dropout(DROPOUT_RATE)(x)\n        x = Dense(128, activation='relu')(x)\n        preds = Dense(2, activation='softmax')(x)\n        return sequence_input, preds\n\n    # Compile model.\n    input_layer, output_layer = get_convolutional_neural_net_layers()\n    model = Model(input_layer, output_layer)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=RMSprop(lr=LEARNING_RATE),\n                  metrics=['acc'])\n\n    # Train model.\n    model.fit(train_text,\n              train_labels,\n              batch_size=BATCH_SIZE,\n              epochs=NUM_EPOCHS,\n              validation_data=(validate_text, validate_labels),\n              verbose=2)\n\n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "19b7e1cbff2165c505dab38c73173a59d4d168e8",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "MODEL_NAME_V1 = 'fat_star_tutorial_v1'\ntokenizer_v1 = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer_v1.fit_on_texts(train_v1_df[TEXT_COLUMN])\nmodel_v1 = train_model(train_v1_df, validate_df, tokenizer_v1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "14d0c646eaa52aec3c7fe50b07c7ef6cb1653051"
      },
      "cell_type": "markdown",
      "source": "## Score test set with the new model\n\nUsing our new model, we can score the set of test comments for toxicity."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2c3443f80de13f4a050b8b0ad87bbacfd72d368f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "test_comments_padded = pad_text(test_df[TEXT_COLUMN], tokenizer_v1)\ntest_df[MODEL_NAME_V1] = model_v1.predict(test_comments_padded)[:, 1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6f10fa7e9b44a2c86e8d087ee1a3356c3c36e3a7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Print some records to compare our model resulsts with the correct labels\ntest_df[[TOXICITY_COLUMN, TEXT_COLUMN, MODEL_NAME_V1]].head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7f8fbd20316a046a8c81c02965b96e36a72c33b8"
      },
      "cell_type": "markdown",
      "source": "## Measure bias\n\nUsing metrics based on Pinned AUC and the Mann Whitney U test, we can measure our model for biases against different identity groups.  We only calculate bias metrics on identities that are refered to in 100 or more comments, to minimize noise."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1b0d78c7a26fe82c751a6abc2bba2523f6117208",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Get a list of identity columns that have >= 100 True records.  This will remove groups such\n# as \"other_disability\" which do not have enough records to calculate meaningful metrics.\nidentities_with_over_100_records = []\nfor identity in identity_columns:\n    num_records = len(test_df.query(identity + '==True'))\n    if num_records >= 100:\n        identities_with_over_100_records.append(identity)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0f21dbb767097a9108d28e581a19a6cfa65c9db0",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def compute_normalized_pinned_auc(df, subgroup, model_name):\n    subgroup_non_toxic = df[df[subgroup] & ~df[TOXICITY_COLUMN]]\n    subgroup_toxic = df[df[subgroup] & df[TOXICITY_COLUMN]]\n    background_non_toxic = df[~df[subgroup] & ~df[TOXICITY_COLUMN]]\n    background_toxic = df[~df[subgroup] & df[TOXICITY_COLUMN]]\n    \n    within_subgroup_mwu = normalized_mwu(subgroup_non_toxic, subgroup_toxic, model_name)\n    cross_negative_mwu = normalized_mwu(subgroup_non_toxic, background_toxic, model_name)\n    cross_positive_mwu = normalized_mwu(background_non_toxic, subgroup_toxic, model_name)\n    \n    return np.mean([1 - within_subgroup_mwu, 1 - cross_negative_mwu, 1 - cross_positive_mwu])\n\ndef normalized_mwu(data1, data2, model_name):\n    \"\"\"Returns the number of pairs where the datapoint in data1 has a greater score than that from data2.\"\"\" \n    scores_1 = data1[model_name]\n    scores_2 = data2[model_name]\n    n1 = len(scores_1)\n    n2 = len(scores_2)\n    u, _ = stats.mannwhitneyu(scores_1, scores_2, alternative = 'less')\n    return u/(n1*n2)\n\ndef compute_pinned_auc(df, identity, model_name):\n    # Create combined_df, containing an equal number of comments that refer to the identity, and\n    # that belong to the background distribution.\n    identity_df = df[df[identity]]\n    nonidentity_df = df[~df[identity]].sample(len(identity_df), random_state=25)\n    combined_df = pd.concat([identity_df, nonidentity_df])\n\n    # Calculate the Pinned AUC\n    true_labels = combined_df[TOXICITY_COLUMN]\n    predicted_labels = combined_df[model_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)\n\ndef get_bias_metrics(df, model_name):\n    bias_metrics_df = pd.DataFrame({\n        'subgroup': identities_with_over_100_records,\n        'pinned_auc': [compute_pinned_auc(df, identity, model_name)\n                       for identity in identities_with_over_100_records],\n        'normalized_pinned_auc': [compute_normalized_pinned_auc(df, identity, model_name)\n                                  for identity in identities_with_over_100_records]\n    })\n    # Re-order columns and sort bias metrics\n    return bias_metrics_df[['subgroup', 'pinned_auc', 'normalized_pinned_auc']].sort_values('pinned_auc')\n\ndef calculate_overall_auc(df, model_name):\n    true_labels = df[TOXICITY_COLUMN]\n    predicted_labels = df[model_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5fc53890039a598396a3da708477764f8bd00443",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "bias_metrics_df = get_bias_metrics(test_df, MODEL_NAME_V1)\nbias_metrics_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c4a10e40cd0f2a2b4b084e854b60ae9a31ccd345",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "calculate_overall_auc(test_df, MODEL_NAME_V1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "225a9efdaacbd45a05ddc204f000264cb09adf4a"
      },
      "cell_type": "markdown",
      "source": "We can graph a histogram of comment scores in each identity.  In the following graphs, the X axis represents the toxicity score given by our new model, and the Y axis represents the comment count.  Blue values are comment whose true label is non-toxic, while red values are those whose true label is toxic.\n\nWe can see that for some identities such as Asian, the model scores most non-toxic comments as less than 0.2 and most toxic comments as greater than 0.2.  This indicates that for the Asian identity, our model is able to distinguish between toxic and non-toxic comments.  However, for the black identity, there are many non-toxic comments with scores over 0.5, along with many toxic comments with scores of less than 0.5.  This shows that for the black identity, our model will be less accurate at separating toxic comments from non-toxic comments."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb54e367f4f7192fad44888e2cb76eaa52ea03db",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Plot toxicity distributions of different identities to visualize bias.\ndef plot_histogram(identity):\n    toxic_scores = test_df.query(identity + ' == True & toxicity == True')[MODEL_NAME_V1]\n    non_toxic_scores = test_df.query(identity + ' == True & toxicity == False')[MODEL_NAME_V1]\n    sns.distplot(non_toxic_scores, color=\"skyblue\", axlabel=identity)\n    sns.distplot(toxic_scores, color=\"red\", axlabel=identity)\n    plt.figure()\n\nfor identity in bias_metrics_df['subgroup']:\n    plot_histogram(identity)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b1a436b033c13dab9884b5ce562efbf47c4da36d"
      },
      "cell_type": "markdown",
      "source": "## Train a new model on more data to reduce some bias\n\nWhile not the only source of bias, one possible explanation for our models poor performance on certain identities is a lack of properly balanced training data across identities. For example, if we query comments labeled as \"homosexual\\_gay\\_or\\_lesbian\" in our initial training data, we see that a larger proportion of those comments are rated toxic than in the entire dataset.  We can try to fix this imbalance and re-train our model using additional training data that contains more non-toxic comments labeled as \"homosexual\\_gay\\_or\\_lesbian\".  After re-training, we hope to improve the Pinned AUC score for this identity."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fa8afbefcad4962069c08b910b0914bd6cd15354",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Load new training data and convert fields to booleans.\ntrain_v2_df = pd.read_csv('../input/fat-star-tutorial-data/public_train_v2.csv')\nfor col in ['toxicity'] + identity_columns:\n    convert_to_bool(train_v2_df, col)\n\n# Create a new model using the same structure as our model_v1.\nMODEL_NAME_V2 = 'fat_star_tutorial_v2'\ntokenizer_v2 = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer_v2.fit_on_texts(train_v2_df[TEXT_COLUMN])\nmodel_v2 = train_model(train_v2_df, validate_df, tokenizer_v2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0c320eae2e9a58d6e63cc5104fd9e3af439e2f28",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "test_comments_padded_v2 = pad_text(test_df[TEXT_COLUMN], tokenizer_v2)\ntest_df[MODEL_NAME_V2] = model_v2.predict(test_comments_padded_v2)[:, 1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "08f6145298c65cda96a2db11d2dcf60a7d1e06af",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "bias_metrics_v2_df = get_bias_metrics(test_df, MODEL_NAME_V2)\nbias_metrics_v2_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f5241d287d1a521a9d3a41cb663d102cc458867b",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}